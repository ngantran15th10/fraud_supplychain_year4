======================================================================
CONSOLIDATED EVALUATION RESULTS - COMBINED MODEL
Supply Chain Fraud Detection Using Deep Neural Networks
======================================================================

Dataset: DataCo Supply Chain Dataset
Total Orders: 180,519
Total Customers: 20,652
Fraud Rate: 6.9% (12,437 fraudulent orders)
Test Set Size: 4,131 orders (3,845 Not Fraud, 286 Fraud)

Feature Engineering:
- Transaction Features: 57 features (order details, customer behavior, shipping)
- Network Features: 4 features (degree, betweenness, closeness, pagerank)
- Combined Features: 61 total features
- Dimensionality Reduction: PCA to 45 components
- Sampling Strategy: SMOTE with sampling_strategy=1.0 (fully balanced training set)

======================================================================
SECTION 1: MODEL CONFIGURATIONS TESTED
======================================================================

Three distinct approaches were evaluated:

1. Single Model (Baseline)
   - Architecture: Deep Neural Network (256->128->64->1)
   - Loss Function: Binary Crossentropy
   - Threshold: Auto-optimized (0.461)
   - Training: Single random seed

2. Ensemble Model (AGGRESSIVE Configuration)
   - Architecture: 3 independent DNNs (256->128->64->1)
   - Loss Function: Cost-Sensitive Focal Loss (FN_COST=15.0)
   - Random Seeds: [42, 123, 456]
   - Prediction: Simple averaging of 3 model outputs
   - Threshold: 0.20 (manually tuned for maximum Recall)
   - Key Innovation: Heavy penalty for False Negatives (FN_COST=15)

3. Stacking Ensemble
   - Base Models: DNN, Random Forest, XGBoost, LightGBM
   - Meta-Learner: Logistic Regression
   - Threshold: 0.05
   - Training: Sequential stacking with cross-validation

======================================================================
SECTION 2: COMPARATIVE PERFORMANCE ANALYSIS
======================================================================

------------------------------------------------------------------------
MODEL 1: ENSEMBLE (AGGRESSIVE) - RECOMMENDED FOR PRODUCTION
------------------------------------------------------------------------

Configuration:
  Number of Models: 3
  Random Seeds: [42, 123, 456]
  Threshold: 0.200
  Loss Function: Cost-Sensitive Focal Loss (gamma=0.8, alpha=0.80, FN_COST=15.0)

Performance Metrics:
  Accuracy:  0.7773
  Precision: 0.2015
  Recall:    0.7483 (TARGET ACHIEVED: >70%)
  F1-Score:  0.3175
  ROC-AUC:   0.8216

Confusion Matrix:
  [[2997  848]   True Negatives: 2997   False Positives: 848
   [  72  214]]  False Negatives: 72    True Positives: 214

Classification Report:
              precision    recall  f1-score   support

   Not Fraud       0.98      0.78      0.87      3845
       Fraud       0.20      0.75      0.32       286

    accuracy                           0.78      4131
   macro avg       0.59      0.76      0.59      4131
weighted avg       0.92      0.78      0.83      4131

Fraud Detection Performance:
  - Frauds Detected: 214 out of 286 (74.83%)
  - Frauds Missed: 72 (25.17%)
  - False Alerts: 848 out of 3845 legitimate orders (22.05%)
  - Alert Rate: 1062 alerts / 4131 orders = 25.7%

INTERPRETATION:
This model represents the optimal configuration for fraud detection in
supply chain transactions. The high Recall (74.83%) indicates that the
model successfully identifies approximately 3 out of 4 fraudulent 
transactions, significantly exceeding the industry target of 70%. 

The relatively low Precision (20.15%) means that only 1 in 5 alerts
corresponds to actual fraud. However, this trade-off is acceptable and
even desirable in fraud detection scenarios where the cost of missing
fraud (False Negatives) far outweighs the cost of investigating false
alarms (False Positives).

The ROC-AUC score of 82.16% demonstrates strong discriminative ability,
indicating that the model can effectively distinguish between fraudulent
and legitimate transactions across various threshold settings.

------------------------------------------------------------------------
MODEL 2: SINGLE MODEL (BASELINE)
------------------------------------------------------------------------

Configuration:
  Architecture: Deep Neural Network (256->128->64->1)
  Loss Function: Binary Crossentropy
  Threshold: Auto-optimized (0.461)
  Random Seed: 42

Performance Metrics:
  Accuracy:  0.8862
  Precision: 0.2820
  Recall:    0.4161 (BELOW TARGET: <70%)
  F1-Score:  0.3362
  ROC-AUC:   0.8205

Confusion Matrix:
  [[3542  303]   True Negatives: 3542   False Positives: 303
   [ 167  119]]  False Negatives: 167   True Positives: 119

Classification Report:
              precision    recall  f1-score   support

   Not Fraud       0.95      0.92      0.94      3845
       Fraud       0.28      0.42      0.34       286

    accuracy                           0.89      4131
   macro avg       0.62      0.67      0.64      4131
weighted avg       0.91      0.89      0.90      4131

Fraud Detection Performance:
  - Frauds Detected: 119 out of 286 (41.61%)
  - Frauds Missed: 167 (58.39%)
  - False Alerts: 303 out of 3845 legitimate orders (7.88%)
  - Alert Rate: 422 alerts / 4131 orders = 10.2%

INTERPRETATION:
The single model baseline demonstrates higher Accuracy (88.62%) and
Precision (28.20%) compared to the ensemble model, but critically fails
to achieve the target Recall of 70%. With a Recall of only 41.61%, this
model misses more than half of all fraudulent transactions, making it
unsuitable for production deployment in fraud detection scenarios.

The auto-optimized threshold (0.461) prioritizes F1-Score optimization,
which balances Precision and Recall. However, this approach is inappropriate
for fraud detection where Recall is the primary objective. The high number
of False Negatives (167) represents significant financial risk.

------------------------------------------------------------------------
MODEL 3: STACKING ENSEMBLE - NOT RECOMMENDED
------------------------------------------------------------------------

Configuration:
  Base Models: DNN, Random Forest, XGBoost, LightGBM
  Meta-Learner: Logistic Regression
  Threshold: 0.05

Performance Metrics:
  Accuracy:  0.9245
  Precision: 0.4393
  Recall:    0.3287 (CRITICALLY BELOW TARGET)
  F1-Score:  0.3760
  ROC-AUC:   0.8490

Confusion Matrix:
  [[3725  120]   True Negatives: 3725   False Positives: 120
   [ 192   94]]  False Negatives: 192   True Positives: 94

Classification Report:
              precision    recall  f1-score   support

   Not Fraud       0.95      0.97      0.96      3845
       Fraud       0.44      0.33      0.38       286

    accuracy                           0.92      4131
   macro avg       0.70      0.65      0.67      4131
weighted avg       0.92      0.92      0.92      4131

Fraud Detection Performance:
  - Frauds Detected: 94 out of 286 (32.87%)
  - Frauds Missed: 192 (67.13%)
  - False Alerts: 120 out of 3845 legitimate orders (3.12%)
  - Alert Rate: 214 alerts / 4131 orders = 5.2%

Base Models Comparison:
        Model  Accuracy  Precision   Recall  F1-Score  ROC-AUC
          DNN  0.103849   0.071715 1.000000  0.133832 0.465465
Random Forest  0.291697   0.089041 1.000000  0.163522 0.830090
      XGBoost  0.748245   0.177778 0.727273  0.285714 0.823895
     LightGBM  0.699831   0.168289 0.846154  0.280742 0.864038
     STACKING  0.924473   0.439252 0.328671  0.376000 0.849035

Meta-Learner Coefficients:
  DNN:      -1.6509 (negative weight - model excluded)
  RF:       +3.1546
  XGBoost:  +10.1465 (highest weight)
  LightGBM: +4.2663

INTERPRETATION:
The stacking ensemble achieves the highest Accuracy (92.45%) and Precision
(43.93%) among all tested models, indicating superior performance in
minimizing false alarms. However, this comes at a severe cost to Recall,
which drops to only 32.87% - the worst performance across all configurations.

This failure stems from the meta-learner's optimization strategy. The
Logistic Regression meta-learner prioritizes overall Accuracy and Precision,
making it overly conservative in fraud prediction. As a result, the model
misses 192 out of 286 fraudulent transactions (67.13%), representing
unacceptable financial risk.

The negative coefficient for the DNN base model (-1.6509) indicates that
the meta-learner actively suppresses DNN predictions, which tend to be
more sensitive to fraud. This conservative behavior makes stacking
unsuitable for fraud detection applications where Recall is paramount.

======================================================================
SECTION 3: SUMMARY AND RECOMMENDATIONS
======================================================================

------------------------------------------------------------------------
COMPARATIVE SUMMARY TABLE
------------------------------------------------------------------------

Metric                    | Ensemble (AGGRESSIVE) | Single Model | Stacking
--------------------------|----------------------|--------------|----------
Accuracy                  |      77.73%          |   88.62%     |  92.45%
Precision                 |      20.15%          |   28.20%     |  43.93%
Recall (PRIMARY METRIC)   |      74.83% ***      |   41.61%     |  32.87%
F1-Score                  |      31.75%          |   33.62%     |  37.60%
ROC-AUC                   |      82.16%          |   82.05%     |  84.90%
Frauds Detected           |      214/286         |   119/286    |   94/286
Frauds Missed             |      72              |   167        |   192
False Positives           |      848             |   303        |   120
Alert Rate                |      25.7%           |   10.2%      |   5.2%
Target Achievement (>70%) |      YES             |   NO         |   NO

*** Indicates best performance on primary metric (Recall)

------------------------------------------------------------------------
FINAL RECOMMENDATION: ENSEMBLE (AGGRESSIVE)
------------------------------------------------------------------------

The Ensemble model with AGGRESSIVE configuration is recommended for
production deployment based on the following rationale:

1. TARGET ACHIEVEMENT:
   Recall of 74.83% exceeds the industry-standard target of 70% for
   fraud detection systems, ensuring that approximately 3 out of 4
   fraudulent transactions are successfully identified.

2. RISK MITIGATION:
   With only 72 False Negatives (25.17% miss rate), the model minimizes
   the financial risk associated with undetected fraud. In contrast, the
   Single Model misses 167 frauds and Stacking misses 192 frauds.

3. ACCEPTABLE TRADE-OFFS:
   While Precision is relatively low (20.15%), this is an acceptable
   trade-off in fraud detection. The 848 False Positives represent
   investigation overhead, but this cost is justified by the superior
   fraud detection rate.

4. ROBUST GENERALIZATION:
   The ensemble approach using 3 independent models with different random
   seeds provides better generalization and reduces overfitting risk
   compared to single model approaches.

5. DISCRIMINATIVE PERFORMANCE:
   ROC-AUC of 82.16% demonstrates strong ability to distinguish between
   fraudulent and legitimate transactions, indicating reliable model
   performance across various operational thresholds.

------------------------------------------------------------------------
WHY OTHER MODELS FAIL
------------------------------------------------------------------------

SINGLE MODEL:
- Recall of 41.61% is critically insufficient for fraud detection
- Misses 167 out of 286 frauds (58.39% miss rate)
- Auto-optimized threshold prioritizes F1-Score over Recall
- Unacceptable financial risk from high False Negative rate

STACKING ENSEMBLE:
- Recall of 32.87% represents catastrophic failure in fraud detection
- Misses 192 out of 286 frauds (67.13% miss rate)
- Meta-learner optimizes for Accuracy/Precision, not Recall
- Overly conservative predictions unsuitable for fraud scenarios
- Despite highest Accuracy (92.45%), fails on primary objective

------------------------------------------------------------------------
COST-BENEFIT ANALYSIS
------------------------------------------------------------------------

Assuming:
- Average fraud transaction value: $1,000
- Investigation cost per alert: $50
- Cost of missed fraud: 100% loss ($1,000 per fraud)

ENSEMBLE (AGGRESSIVE):
  Frauds Caught: 214 × $1,000 = $214,000 saved
  Frauds Missed: 72 × $1,000 = $72,000 lost
  Investigation Cost: 1,062 alerts × $50 = $53,100
  Net Benefit: $214,000 - $72,000 - $53,100 = $88,900

SINGLE MODEL:
  Frauds Caught: 119 × $1,000 = $119,000 saved
  Frauds Missed: 167 × $1,000 = $167,000 lost
  Investigation Cost: 422 alerts × $50 = $21,100
  Net Benefit: $119,000 - $167,000 - $21,100 = -$69,100 (NET LOSS)

STACKING:
  Frauds Caught: 94 × $1,000 = $94,000 saved
  Frauds Missed: 192 × $1,000 = $192,000 lost
  Investigation Cost: 214 alerts × $50 = $10,700
  Net Benefit: $94,000 - $192,000 - $10,700 = -$108,700 (SEVERE LOSS)

The Ensemble model is the only configuration that achieves positive
net benefit, justifying its deployment in production environments.

======================================================================
SECTION 4: TECHNICAL IMPLEMENTATION DETAILS
======================================================================

ENSEMBLE (AGGRESSIVE) - Production Configuration:

Neural Network Architecture:
  - Input Layer: 45 features (after PCA)
  - Hidden Layer 1: 256 neurons, BatchNormalization, Dropout(0.3), ReLU
  - Hidden Layer 2: 128 neurons, BatchNormalization, Dropout(0.3), ReLU
  - Hidden Layer 3: 64 neurons, BatchNormalization, Dropout(0.2), ReLU
  - Output Layer: 1 neuron, Sigmoid activation

Loss Function: Cost-Sensitive Focal Loss
  - Gamma: 0.8 (focus on hard examples)
  - Alpha: 0.80 (class balance weight)
  - FN_COST: 15.0 (heavy penalty for False Negatives)
  - Formula: FL + (y_true * (1 - y_pred) * FN_COST)

Training Configuration:
  - Optimizer: Adam
  - Batch Size: 32
  - Epochs: 50 with EarlyStopping (patience=10)
  - Learning Rate: 0.001

Data Preprocessing:
  - SMOTE: sampling_strategy=1.0 (full balance: 1:1 fraud to non-fraud)
  - PCA: 61 features -> 45 components (explained variance >95%)
  - Standardization: StandardScaler on all features

Ensemble Strategy:
  - Model 1: Random seed 42
  - Model 2: Random seed 123
  - Model 3: Random seed 456
  - Prediction: Average of 3 model outputs
  - Final Classification: Threshold = 0.20

======================================================================
SECTION 5: CONCLUSION
======================================================================

This comprehensive evaluation demonstrates that the Ensemble model with
AGGRESSIVE configuration significantly outperforms both baseline and
alternative ensemble approaches for supply chain fraud detection.

Key Achievements:
1. Recall of 74.83% exceeds industry target (>70%)
2. ROC-AUC of 82.16% indicates strong discriminative performance
3. Cost-Sensitive Focal Loss effectively addresses class imbalance
4. Ensemble approach provides robust generalization
5. Positive net benefit ($88,900) justifies production deployment

The superior performance stems from three critical design decisions:
(1) Cost-sensitive loss function that heavily penalizes False Negatives,
(2) Low threshold (0.20) optimized for maximum Recall rather than F1-Score,
(3) Ensemble averaging to reduce variance and improve generalization.

This model is ready for production deployment in supply chain fraud
detection systems where maximizing fraud detection rate is the primary
objective, and the cost of investigation is acceptable relative to the
cost of undetected fraud.

======================================================================
END OF REPORT
Generated: November 4, 2025
Model Version: Combined Model (Transaction + Network Features)
======================================================================
